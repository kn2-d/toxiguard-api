"""
AIåˆ†æã‚µãƒ¼ãƒ“ã‚¹ï¼ˆrinnaæ—¥æœ¬èªAIä½¿ç”¨ï¼‰
æ¯’æ€§æ¤œçŸ¥ã®AIåˆ¤å®šæ©Ÿèƒ½ã‚’æä¾›
"""

from typing import Dict, Optional
import torch
from transformers import AutoTokenizer, AutoModel
import logging
from pathlib import Path

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RinnaAnalyzer:
    """rinnaæ—¥æœ¬èªAIã‚’ä½¿ç”¨ã—ãŸæ¯’æ€§åˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–ï¼šãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æº–å‚™"""
        self.model_name = "rinna/japanese-roberta-base"
        self.device = "cpu"  # GPUä½¿ç”¨æ™‚ã¯"cuda"
        self.model = None
        self.tokenizer = None
        self.is_loaded = False
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½è¿½åŠ 
        self.cache = {}  # åˆ†æçµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.cache_max_size = 100  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€å¤§ä»¶æ•°
        
        logger.info(f"ğŸ¤– RinnaAnalyzeråˆæœŸåŒ–é–‹å§‹")
        
    def load_model(self) -> bool:
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆé…å»¶èª­ã¿è¾¼ã¿æ–¹å¼ï¼‰"""
        try:
            logger.info(f"ğŸ“¥ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿é–‹å§‹: {self.model_name}")
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            self.model = AutoModel.from_pretrained(self.model_name)
            self.model.to(self.device)
            self.model.eval()  # æ¨è«–ãƒ¢ãƒ¼ãƒ‰
            
            self.is_loaded = True
            logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {str(e)}")
            return False
    
    def get_model_info(self) -> Dict[str, str]:
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—"""
        if not self.is_loaded:
            return {"status": "æœªèª­ã¿è¾¼ã¿"}
            
        return {
            "model_name": self.model_name,
            "device": self.device,
            "parameters": f"{self.model.num_parameters():,}",
            "status": "èª­ã¿è¾¼ã¿æ¸ˆã¿"
        }
    
    async def analyze_text(self, text: str) -> Dict[str, float]:
        """ãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œç‰ˆï¼‰"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        cache_key = text.strip().lower()
        if cache_key in self.cache:
            logger.info(f"ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {text[:20]}...")
            return self.cache[cache_key]
        
        # ãƒ¢ãƒ‡ãƒ«æœªèª­ã¿è¾¼ã¿ã®å ´åˆã¯èª­ã¿è¾¼ã¿
        if not self.is_loaded:
            if not self.load_model():
                return {"error": "ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—", "score": 0.0}
        
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã¨ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            logger.info(f"ğŸ” åˆ†æé–‹å§‹: {text[:20]}...")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=True
            )
            
            # AIæ¨è«–å®Ÿè¡Œ
            with torch.no_grad():
                outputs = self.model(**inputs)
                
            # æ¯’æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
            toxicity_score = self._calculate_toxicity_score(outputs, text)
            
            # çµæœä½œæˆ
            result = {
                "ai_score": toxicity_score,
                "confidence": 0.8,
                "model_used": self.model_name
            }
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆã‚µã‚¤ã‚ºåˆ¶é™ä»˜ãï¼‰
            if len(self.cache) >= self.cache_max_size:
                # å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤ï¼ˆFIFOæ–¹å¼ï¼‰
                oldest_key = next(iter(self.cache))
                del self.cache[oldest_key]
                logger.info(f"ğŸ§¹ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢: {oldest_key[:20]}...")
            
            self.cache[cache_key] = result
            logger.info(f"âœ… åˆ†æå®Œäº†ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {"error": str(e), "score": 0.0}
    
    def _calculate_toxicity_score(self, model_outputs, text: str) -> float:
        """AIå‡ºåŠ›ã‹ã‚‰æ¯’æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        try:
            # éš ã‚ŒçŠ¶æ…‹ã®å–å¾—
            last_hidden_state = model_outputs.last_hidden_state
            
            # CLSãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ–‡å…¨ä½“ã®è¡¨ç¾ï¼‰ã‚’ä½¿ç”¨
            cls_embedding = last_hidden_state[0, 0, :]  # [batch=0, token=0, features]
            
            # æ¯’æ€§åˆ¤å®šã®ç‰¹å¾´é‡è¨ˆç®—
            # 1. ãƒã‚¬ãƒ†ã‚£ãƒ–è¦ç´ ã®å¼·åº¦
            negative_intensity = torch.mean(torch.abs(cls_embedding)).item()
            
            # 2. æ–‡ã®é•·ã•è£œæ­£
            text_length = len(text)
            length_factor = min(1.0, text_length / 10.0)  # çŸ­æ–‡ã¯ä¿‚æ•°å°
            
            # 3. åŸºæœ¬æ¯’æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
            base_score = min(1.0, negative_intensity * 0.1)  # 0-1æ­£è¦åŒ–
            
            # 4. æ¯’æ€§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒœãƒ¼ãƒŠã‚¹ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            toxic_keywords = ["æ­»ã­", "æ®ºã™", "ã‚¯ã‚º", "ãƒã‚«", "ã‚¢ãƒ›"]
            keyword_bonus = 0.0
            for keyword in toxic_keywords:
                if keyword in text:
                    keyword_bonus += 0.3
            
            # 5. æœ€çµ‚ã‚¹ã‚³ã‚¢è¨ˆç®—
            final_score = min(1.0, (base_score + keyword_bonus) * length_factor)
            
            logger.info(f"ğŸ§® ã‚¹ã‚³ã‚¢è©³ç´° - base:{base_score:.3f}, bonus:{keyword_bonus:.3f}, final:{final_score:.3f}")
            
            return final_score
            
        except Exception as e:
            logger.error(f"âŒ ã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return 0.0
    
    async def analyze_batch(self, texts: list) -> Dict[str, Dict]:
        """è¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆã®ä¸€æ‹¬åˆ†æï¼ˆé«˜é€ŸåŒ–ï¼‰"""
        results = {}
        
        logger.info(f"ğŸ“¦ ãƒãƒƒãƒåˆ†æé–‹å§‹: {len(texts)}ä»¶")
        
        for i, text in enumerate(texts):
            result = await self.analyze_text(text)
            results[f"text_{i}"] = {
                "input": text,
                "result": result
            }
        
        logger.info(f"âœ… ãƒãƒƒãƒåˆ†æå®Œäº†")
        return results

    def get_cache_stats(self) -> Dict[str, int]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆæƒ…å ±"""
        return {
            "cache_size": len(self.cache),
            "max_size": self.cache_max_size,
            "hit_rate": "å®Ÿè£…äºˆå®š"  # å°†æ¥ã®æ©Ÿèƒ½
        }


# ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°
async def test_analyzer():
    """å‹•ä½œç¢ºèªç”¨ãƒ†ã‚¹ãƒˆ"""
    analyzer = RinnaAnalyzer()
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º
    info = analyzer.get_model_info()
    print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±: {info}")
    
    # ãƒ†ã‚¹ãƒˆåˆ†æ
    test_texts = ["ã“ã‚“ã«ã¡ã¯", "æ­»ã­", "è‰¯ã„å¤©æ°—ã§ã™ã­"]
    
    for text in test_texts:
        result = await analyzer.analyze_text(text)
        print(f"å…¥åŠ›: {text} â†’ çµæœ: {result}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(test_analyzer())