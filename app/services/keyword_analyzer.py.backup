"""
キーワードベースの毒性分析サービス
リリース1のコア機能
"""
import json
import re
from pathlib import Path
from typing import Dict, List, Tuple
from app.models.schemas import ToxicityCategory
import logging

# ログの設定（デバッグ用）
logger = logging.getLogger(__name__)

class KeywordAnalyzer:
    """キーワードベースの毒性分析器"""
    
    def __init__(self):
        """初期化：キーワードデータを読み込む"""
        self.data = self._load_keywords()
        self.categories = self.data["categories"]
        self.modifiers = self.data["modifiers"]
        logger.info("KeywordAnalyzer initialized")
        
    def _load_keywords(self) -> Dict:
        """
        キーワードデータを読み込む
        toxic_keywords.jsonファイルを読み込んでPythonの辞書に変換
        """
        # ファイルパスを構築（このファイルから見た相対パス）
        data_path = Path(__file__).parent.parent / "data" / "toxic_keywords.json"
        
        # JSONファイルを読み込む
        with open(data_path, "r", encoding="utf-8") as f:
            return json.load(f)
    
    def analyze(self, text: str) -> Tuple[float, List[ToxicityCategory], float]:
        """
        テキストを分析して毒性スコアを返す
        
        Args:
            text: 分析対象のテキスト
            
        Returns:
            tuple: (毒性スコア, カテゴリリスト, 信頼度)
        """
        # 1. テキストの前処理（正規化）
        normalized_text = self._normalize_text(text)
        
        # 2. カテゴリ別の分析
        categories_result = []  # 結果を入れるリスト
        total_score = 0.0      # 合計スコア
        max_score = 0.0        # 最大スコア
        
        # 各カテゴリをチェック
        for category_id, category_data in self.categories.items():
            # カテゴリごとに分析
            score, found_keywords = self._analyze_category(
                normalized_text, 
                category_data
            )
            
            # スコアが0より大きい場合は結果に追加
            if score > 0:
                categories_result.append(ToxicityCategory(
                    name=category_data["name"],
                    score=score,
                    keywords_found=found_keywords
                ))
                
                # 重み付けスコアの計算
                weighted_score = score * category_data["weight"]
                total_score += weighted_score
                max_score = max(max_score, weighted_score)
        
        # 3. 最終スコアの計算
        if len(categories_result) > 0:
            # 最大値70% + 平均値30%の組み合わせ
            avg_score = total_score / len(self.categories)
            final_score = (max_score * 0.7) + (avg_score * 0.3)
            final_score = min(1.0, final_score)  # 1.0を超えないように
        else:
            final_score = 0.0
        
        # 4. 信頼度の計算
        confidence = self._calculate_confidence(categories_result, len(text))
        
        return final_score, categories_result, confidence
    
    def _normalize_text(self, text: str) -> str:
        """
        テキストの正規化
        大文字小文字の統一、スペースの正規化など
        """
        # 小文字に変換
        text = text.lower()
        
        # 連続するスペースや改行を1つのスペースに
        text = re.sub(r'\s+', ' ', text)
        
        # 全角英数字を半角に変換
        # 変換テーブルを作成
        text = text.translate(str.maketrans(
            'ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ０１２３４５６７８９',
            'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789'
        ))
        
        return text
    
    def _analyze_category(self, text: str, category_data: Dict) -> Tuple[float, List[str]]:
        """
        カテゴリ別の分析
        
        Args:
            text: 正規化されたテキスト
            category_data: カテゴリ情報（キーワードリストなど）
            
        Returns:
            tuple: (スコア, 見つかったキーワードのリスト)
        """
        keywords = category_data["keywords"]
        found_keywords = []
        
        # 各キーワードをチェック
        for keyword in keywords:
            if keyword in text:
                found_keywords.append(keyword)
                
                # 強調表現もチェック（「超死ね」など）
                for intensifier in self.modifiers["intensifiers"]:
                    if f"{intensifier}{keyword}" in text:
                        found_keywords.append(f"{intensifier}{keyword}")
        
        # スコア計算
        if found_keywords:
            # 重複を除去
            unique_keywords = list(set(found_keywords))
            # 3個以上見つかったら最大スコア
            score = min(1.0, len(unique_keywords) / 3)
        else:
            score = 0.0
            
        return score, found_keywords
    
    def _calculate_confidence(self, categories: List[ToxicityCategory], text_length: int) -> float:
        """
        信頼度の計算
        キーワードの数とテキストの長さから信頼度を計算
        
        Args:
            categories: 検出されたカテゴリのリスト
            text_length: テキストの長さ
            
        Returns:
            float: 信頼度（0.5〜1.0）
        """
        if not categories:
            # キーワードが見つからない = おそらく安全
            return 0.9
        
        # キーワード数とテキスト長の比率を計算
        total_keywords = sum(len(cat.keywords_found) for cat in categories)
        # テキスト10文字あたりのキーワード数
        keyword_density = total_keywords / max(text_length / 10, 1)
        
        # 信頼度（0.5〜1.0の範囲）
        confidence = min(0.5 + (keyword_density * 0.5), 1.0)
        
        return confidence
