# app/services/multi_model_analyzer.py
"""
Release 4 ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«çµ±åˆã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼
KeywordAnalyzer + ToxicBertAnalyzer + MistralAnalyzer (Phi-2) + ClaudeAnalyzer
"""
import asyncio
from asyncio import TimeoutError
from typing import Dict, List, Optional, Literal, Tuple
from dataclasses import dataclass
import logging
import time
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed

# å„ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, project_root)

try:
    from app.services.keyword_analyzer import KeywordAnalyzer
    from app.services.toxic_bert_analyzer import ToxicBertAnalyzer
    from app.services.mistral_analyzer import MistralAnalyzer
    from app.services.claude_analyzer import ClaudeAnalyzer  # Release 4ã§è¿½åŠ 
    from app.config import settings, MODEL_CONFIGS
    from app.models.schemas import ToxicityCategory
except ImportError:
    # åˆ¥ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–¹æ³•ã‚’è©¦ã™
    try:
        from keyword_analyzer import KeywordAnalyzer
        from toxic_bert_analyzer import ToxicBertAnalyzer
        from mistral_analyzer import MistralAnalyzer
        from claude_analyzer import ClaudeAnalyzer  # Release 4ã§è¿½åŠ 
        from config import settings, MODEL_CONFIGS
        from models.schemas import ToxicityCategory
    except ImportError as e:
        print(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}")
        print(f"Pythonãƒ‘ã‚¹: {sys.path}")
        raise

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class ModelResult:
    """å„ãƒ¢ãƒ‡ãƒ«ã®åˆ†æçµæœ"""
    model_name: str
    toxicity_score: float
    is_toxic: bool
    categories: List[ToxicityCategory]
    confidence: float
    response_time: float
    error: Optional[str] = None


# app/services/multi_model_analyzer.pyï¼ˆä¿®æ­£ç‰ˆã®è©²å½“éƒ¨åˆ†ï¼‰
class MultiModelAnalyzer:
    """è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ãŸåˆ†æã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–"""
        self.models = {}
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ï¼ˆå¿…é ˆï¼‰
        try:
            self.models['keyword'] = KeywordAnalyzer()
            logger.info("KeywordAnalyzeråˆæœŸåŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"KeywordAnalyzeråˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ToxicBERTã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼
        if 'toxic_bert' in MODEL_CONFIGS and MODEL_CONFIGS['toxic_bert'].enabled:
            try:
                self.models['toxic_bert'] = ToxicBertAnalyzer()
                logger.info("ToxicBertAnalyzeråˆæœŸåŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"ToxicBertAnalyzeråˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        # Mistralã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ï¼ˆPhi-2ï¼‰
        if 'mistral' in MODEL_CONFIGS and MODEL_CONFIGS['mistral'].enabled:
            try:
                self.models['mistral'] = MistralAnalyzer()
                logger.info("MistralAnalyzeråˆæœŸåŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"MistralAnalyzeråˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        # Claude ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ï¼ˆRelease 4ã§è¿½åŠ ï¼‰
        if 'claude' in MODEL_CONFIGS and MODEL_CONFIGS['claude'].enabled:
            try:
                self.models['claude'] = ClaudeAnalyzer()
                logger.info("ClaudeAnalyzeråˆæœŸåŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"ClaudeAnalyzeråˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        # OpenAI ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰
        if 'openai' in MODEL_CONFIGS and MODEL_CONFIGS['openai'].enabled:
            logger.info("OpenAIAnalyzer: æœªå®Ÿè£…")
        
        # Google ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰
        if 'google' in MODEL_CONFIGS and MODEL_CONFIGS['google'].enabled:
            logger.info("GoogleAnalyzer: æœªå®Ÿè£…")
        
        logger.info(f"åˆæœŸåŒ–å®Œäº†: {list(self.models.keys())}")
        
        # ãƒ¢ãƒ‡ãƒ«é‡ã¿ï¼ˆconfig.pyã‹ã‚‰å–å¾—ï¼‰
        self.model_weights = {
            'keyword': MODEL_CONFIGS['keyword'].weight if 'keyword' in MODEL_CONFIGS else 0.2,
            'toxic_bert': MODEL_CONFIGS['toxic_bert'].weight if 'toxic_bert' in MODEL_CONFIGS else 0.3,
            'mistral': MODEL_CONFIGS['mistral'].weight if 'mistral' in MODEL_CONFIGS else 0.15,
            'claude': MODEL_CONFIGS['claude'].weight if 'claude' in MODEL_CONFIGS else 0.2,
            'openai': MODEL_CONFIGS['openai'].weight if 'openai' in MODEL_CONFIGS else 0.15,
            'google': MODEL_CONFIGS['google'].weight if 'google' in MODEL_CONFIGS else 0.0,
        }
        
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šï¼ˆç§’ï¼‰
        self.timeouts = {
            'keyword': MODEL_CONFIGS['keyword'].timeout if 'keyword' in MODEL_CONFIGS else 1,
            'toxic_bert': MODEL_CONFIGS['toxic_bert'].timeout if 'toxic_bert' in MODEL_CONFIGS else 3,
            'mistral': MODEL_CONFIGS['mistral'].timeout if 'mistral' in MODEL_CONFIGS else 5,
            'claude': MODEL_CONFIGS['claude'].timeout if 'claude' in MODEL_CONFIGS else 10,
            'openai': MODEL_CONFIGS['openai'].timeout if 'openai' in MODEL_CONFIGS else 10,
            'google': MODEL_CONFIGS['google'].timeout if 'google' in MODEL_CONFIGS else 5,
        }
    
    async def analyze_with_model(self, model_name: str, text: str) -> ModelResult:
        """å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã§åˆ†æã‚’å®Ÿè¡Œ"""
        if model_name not in self.models:
            return ModelResult(
                model_name=model_name,
                toxicity_score=0.0,
                is_toxic=False,
                categories=[],
                confidence=0.0,
                response_time=0.0,
                error=f"Model {model_name} not available"
            )
        
        start_time = time.time()
        try:
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§å®Ÿè¡Œ
            timeout = self.timeouts.get(model_name, 5)
            
            # éåŒæœŸå®Ÿè¡Œã®ãƒ©ãƒƒãƒ‘ãƒ¼
            async def run_analysis():
                model = self.models[model_name]
                # analyzeãƒ¡ã‚½ãƒƒãƒ‰ã‚’éåŒæœŸã§å®Ÿè¡Œ
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, model.analyze, text)
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§å®Ÿè¡Œ
            score, categories, confidence = await asyncio.wait_for(
                run_analysis(),
                timeout=timeout
            )
            
            response_time = time.time() - start_time
            
            return ModelResult(
                model_name=model_name,
                toxicity_score=score,
                is_toxic=score >= 0.3,
                categories=categories,
                confidence=confidence,
                response_time=response_time
            )
            
        except asyncio.TimeoutError:
            logger.warning(f"{model_name}ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ{self.timeouts[model_name]}ç§’ï¼‰")
            return ModelResult(
                model_name=model_name,
                toxicity_score=0.0,
                is_toxic=False,
                categories=[],
                confidence=0.0,
                response_time=time.time() - start_time,
                error="Timeout"
            )
        except Exception as e:
            logger.error(f"{model_name}ã‚¨ãƒ©ãƒ¼: {e}")
            return ModelResult(
                model_name=model_name,
                toxicity_score=0.0,
                is_toxic=False,
                categories=[],
                confidence=0.0,
                response_time=time.time() - start_time,
                error=str(e)
            )
    
    async def analyze_with_strategy(
        self, 
        text: str, 
        strategy: Literal["fast", "cascade", "balanced", "accurate"] = "balanced"
    ) -> Dict:
        """æˆ¦ç•¥ã«åŸºã¥ã„ã¦åˆ†æã‚’å®Ÿè¡Œ"""
        
        start_time = time.time()
        results = []
        
        if strategy == "fast":
            # æœ€é€Ÿï¼šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿
            result = await self.analyze_with_model("keyword", text)
            results.append(result)
            
        elif strategy == "cascade":
            # æ®µéšçš„ï¼šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ â†’ å¿…è¦ã«å¿œã˜ã¦ä»–ã®ãƒ¢ãƒ‡ãƒ«
            keyword_result = await self.analyze_with_model("keyword", text)
            results.append(keyword_result)
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ä¸­é–“çš„ãªã‚¹ã‚³ã‚¢ã®å ´åˆã€è¿½åŠ åˆ†æ
            if 0.2 <= keyword_result.toxicity_score <= 0.7:
                if "toxic_bert" in self.models:
                    bert_result = await self.analyze_with_model("toxic_bert", text)
                    results.append(bert_result)
                    
                # ã•ã‚‰ã«é«˜ç²¾åº¦ãŒå¿…è¦ãªå ´åˆã€Claudeè¿½åŠ 
                if 0.3 <= keyword_result.toxicity_score <= 0.6 and "claude" in self.models:
                    claude_result = await self.analyze_with_model("claude", text)
                    results.append(claude_result)
        
        elif strategy == "balanced":
            # ãƒãƒ©ãƒ³ã‚¹å‹ï¼šåˆ©ç”¨å¯èƒ½ãªå…¨ãƒ¢ãƒ‡ãƒ«ã‚’ä¸¦åˆ—å®Ÿè¡Œ
            tasks = []
            for model_name in self.models.keys():
                task = asyncio.create_task(self.analyze_with_model(model_name, text))
                tasks.append((model_name, task))
            
            # å…¨ã‚¿ã‚¹ã‚¯ã®å®Œäº†ã‚’å¾…ã¤
            for model_name, task in tasks:
                try:
                    result = await task
                    results.append(result)
                except Exception as e:
                    logger.error(f"{model_name}ã®å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        
        elif strategy == "accurate":
            # é«˜ç²¾åº¦ï¼šé‡è¦ãªãƒ¢ãƒ‡ãƒ«ã«é‡ç‚¹
            priority_models = ["keyword", "toxic_bert", "claude"]
            tasks = []
            
            for model_name in priority_models:
                if model_name in self.models:
                    task = asyncio.create_task(self.analyze_with_model(model_name, text))
                    tasks.append((model_name, task))
            
            # å…¨ã‚¿ã‚¹ã‚¯ã®å®Œäº†ã‚’å¾…ã¤
            for model_name, task in tasks:
                try:
                    result = await task
                    results.append(result)
                except Exception as e:
                    logger.error(f"{model_name}ã®å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        
        # çµæœã®çµ±åˆ
        final_result = self._aggregate_results(results, strategy)
        final_result["analysis_time"] = time.time() - start_time
        final_result["strategy"] = strategy
        final_result["models_used"] = [r.model_name for r in results if r.error is None]
        
        return final_result
    
    def _aggregate_results(self, results: List[ModelResult], strategy: str) -> Dict:
        """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµæœã‚’çµ±åˆ"""
        if not results:
            return {
                "text": "",
                "toxicity_score": 0.0,
                "is_toxic": False,
                "categories": [],
                "confidence": 0.0,
                "details": {"error": "No results"}
            }
        
        # ã‚¨ãƒ©ãƒ¼ã§ãªã„çµæœã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
        valid_results = [r for r in results if r.error is None]
        
        if not valid_results:
            return {
                "text": "",
                "toxicity_score": 0.0,
                "is_toxic": False,
                "categories": [],
                "confidence": 0.0,
                "details": {"error": "All models failed"}
            }
        
        # é‡ã¿ä»˜ãå¹³å‡ã®è¨ˆç®—
        total_weight = 0
        weighted_score = 0
        all_categories = []
        confidence_scores = []
        
        for result in valid_results:
            weight = self.model_weights.get(result.model_name, 0.1)
            weighted_score += result.toxicity_score * weight
            total_weight += weight
            all_categories.extend(result.categories)
            confidence_scores.append(result.confidence)
        
        # æœ€çµ‚ã‚¹ã‚³ã‚¢
        final_score = weighted_score / total_weight if total_weight > 0 else 0.0
        
        # ã‚«ãƒ†ã‚´ãƒªã®çµ±åˆï¼ˆé‡è¤‡ã‚’é™¤ãï¼‰
        unique_categories = []
        seen_names = set()
        for cat in all_categories:
            if cat.name not in seen_names:
                unique_categories.append(cat)
                seen_names.add(cat.name)
        
        # ä¿¡é ¼åº¦ã®è¨ˆç®—
        avg_confidence = np.mean(confidence_scores) if confidence_scores else 0.0
        
        # å„ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°çµæœ
        model_details = {}
        for result in results:
            model_details[result.model_name] = {
                "score": result.toxicity_score,
                "is_toxic": result.is_toxic,
                "confidence": result.confidence,
                "response_time": result.response_time,
                "error": result.error
            }
        
        return {
            "text": "",  # ãƒ†ã‚­ã‚¹ãƒˆã¯å¾Œã§è¨­å®š
            "toxicity_score": final_score,
            "is_toxic": final_score >= 0.3,
            "categories": unique_categories,
            "confidence": avg_confidence,
            "details": {
                "model_results": model_details,
                "aggregation_method": "weighted_average",
                "total_models": len(results),
                "valid_models": len(valid_results)
            }
        }
    
    def get_available_models(self) -> List[str]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
        return list(self.models.keys())
    
    def get_model_info(self) -> Dict:
        """å„ãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’è¿”ã™"""
        info = {}
        for name, model in self.models.items():
            info[name] = {
                "enabled": True,
                "weight": self.model_weights.get(name, 0.1),
                "timeout": self.timeouts.get(name, 5),
                "type": type(model).__name__
            }
        return info


# ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰
if __name__ == "__main__":
    async def test():
        print("=== ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆRelease 4ï¼‰ ===")
        analyzer = MultiModelAnalyzer()
        
        print(f"åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«: {analyzer.get_available_models()}")
        print("\nãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
        for name, info in analyzer.get_model_info().items():
            print(f"  {name}: {info}")
        
        test_cases = [
            "ã“ã‚“ã«ã¡ã¯ã€è‰¯ã„å¤©æ°—ã§ã™ã­",
            "æ­»ã­",
            "ãƒã‚«",
            "ã†ã–ã„ã€æ¶ˆãˆã‚",
        ]
        
        strategies = ["fast", "cascade", "balanced", "accurate"]
        
        for text in test_cases:
            print(f"\n{'='*60}")
            print(f"ãƒ†ã‚­ã‚¹ãƒˆ: {text}")
            print('='*60)
            
            for strategy in strategies:
                print(f"\næˆ¦ç•¥: {strategy}")
                result = await analyzer.analyze_with_strategy(text, strategy)
                result["text"] = text  # ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨­å®š
                
                print(f"ç·åˆã‚¹ã‚³ã‚¢: {result['toxicity_score']:.3f}")
                print(f"æœ‰å®³åˆ¤å®š: {'ğŸš¨ æœ‰å®³' if result['is_toxic'] else 'âœ… å®‰å…¨'}")
                print(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {result['models_used']}")
                print(f"åˆ†ææ™‚é–“: {result['analysis_time']:.2f}ç§’")
                
                if result['categories']:
                    print("æ¤œå‡ºã‚«ãƒ†ã‚´ãƒª:")
                    for cat in result['categories']:
                        print(f"  - {cat.name}")
    
    asyncio.run(test())