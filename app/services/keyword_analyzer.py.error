"""
キーワードベースの毒性分析サービス
リリース1のコア機能
修正版：スコア計算を改善
"""
import json
import re
from pathlib import Path
from typing import Dict, List, Tuple
from app.models.schemas import ToxicityCategory
import logging

# ログの設定（デバッグ用）
logger = logging.getLogger(__name__)

class KeywordAnalyzer:
    """キーワードベースの毒性分析器"""
    
    def __init__(self):
        """初期化：キーワードデータを読み込む"""
        self.data = self._load_keywords()
        self.categories = self.data["categories"]
        self.modifiers = self.data["modifiers"]
        logger.info("KeywordAnalyzer initialized")
        
    def _load_keywords(self) -> Dict:
        """
        キーワードデータを読み込む
        toxic_keywords.jsonファイルを読み込んでPythonの辞書に変換
        """
        # ファイルパスを構築（このファイルから見た相対パス）
        data_path = Path(__file__).parent.parent / "data" / "toxic_keywords.json"
        
        # JSONファイルを読み込む
        with open(data_path, "r", encoding="utf-8") as f:
            return json.load(f)
    
    def analyze(self, text: str) -> Tuple[float, List[ToxicityCategory], float]:
        """
        テキストを分析して毒性スコアを返す
        
        Args:
            text: 分析対象のテキスト
            
        Returns:
            tuple: (毒性スコア, カテゴリリスト, 信頼度)
        """
        # 1. テキストの前処理（正規化）
        normalized_text = self._normalize_text(text)
        
        # 2. カテゴリ別の分析
        categories_result = []  # 結果を入れるリスト
        total_score = 0.0      # 合計スコア
        max_score = 0.0        # 最大スコア
        
        # 各カテゴリをチェック
        for category_id, category_data in self.categories.items():
            # カテゴリごとに分析
            score, found_keywords = self._analyze_category(
                normalized_text, 
                category_data
            )
            
            # スコアが0より大きい場合は結果に追加
            if score > 0:
                categories_result.append(ToxicityCategory(
                    name=category_data["name"],
                    score=score,
                    keywords_found=found_keywords
                ))
                
                # 重み付けスコアの計算
                weighted_score = score * category_data["weight"]
                total_score += weighted_score
                max_score = max(max_score, weighted_score)
        
        # 3. 最終スコアの計算（修正版）
        if len(categories_result) > 0:
            # 検出されたカテゴリのみで平均を計算
            avg_score = total_score / len(categories_result)
            # 最大値80% + 平均値20%の組み合わせ
            final_score = (max_score * 0.8) + (avg_score * 0.2)
            final_score = min(1.0, final_score)  # 1.0を超えないように
        else:
            final_score = 0.0
        
        # 4. 信頼度の計算
        confidence = self._calculate_confidence(categories_result, len(text))
        # デバッグ情報（この3行を追加）
        print(f"DEBUG: text='{text}'")
        print(f"DEBUG: max_score={max_score}, total_score={total_score}")
        print(f"DEBUG: final_score={final_score}, categories={len(categories_result)}")
        
        return final_score, categories_result, confidence
    
    def _normalize_text(self, text: str) -> str:
        """
        テキストの正規化
        大文字小文字の統一、スペースの正規化など
        """
        # 小文字に変換
        text = text.lower()
        
        # 連続するスペースや改行を1つのスペースに
        text = re.sub(r'\s+', ' ', text)
        
        # 全角英数字を半角に変換
        # 変換テーブルを作成
        text = text.translate(str.maketrans(
            'ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ０１２３４５６７８９',
            'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789'
        ))
        
        return text
    
def _analyze_category(self, text: str, category_data: Dict) -> Tuple[float, List[str]]:
    """カテゴリ別の分析"""
    keywords = category_data["keywords"]
    found_keywords = []
    
    # 各キーワードをチェック
    for keyword in keywords:
        if keyword in text:
            found_keywords.append(keyword)
            
            # 強調表現もチェック
            for intensifier in self.modifiers["intensifiers"]:
                if f"{intensifier}{keyword}" in text:
                    found_keywords.append(f"{intensifier}{keyword}")
    
    # スコア計算（さらに修正）
    if found_keywords:
        unique_keywords = list(set(found_keywords))
        
        # カテゴリの重要度に応じてベーススコアを調整
        weight = category_data.get("weight", 0.5)
        
        if weight >= 1.0:  # 重度の毒性
            # 1個でも0.8、2個以上で1.0
            if len(unique_keywords) == 1:
                score = 0.8
            else:
                score = 1.0
        elif weight >= 0.8:  # 中程度の毒性
            # 1個:0.6, 2個:0.8, 3個以上:1.0
            if len(unique_keywords) == 1:
                score = 0.6
            elif len(unique_keywords) == 2:
                score = 0.8
            else:
                score = 1.0
        else:  # 軽度の毒性
            # 1個:0.4, 2個:0.6, 3個以上:0.8
            if len(unique_keywords) == 1:
                score = 0.4
            elif len(unique_keywords) == 2:
                score = 0.6
            else:
                score = 0.8
    else:
        score = 0.0
        
    return score, found_keywords
    
    def _calculate_confidence(self, categories: List[ToxicityCategory], text_length: int) -> float:
        """
        信頼度の計算
        キーワードの数とテキストの長さから信頼度を計算
        
        Args:
            categories: 検出されたカテゴリのリスト
            text_length: テキストの長さ
            
        Returns:
            float: 信頼度（0.5〜1.0）
        """
        if not categories:
            # キーワードが見つからない = おそらく安全
            return 0.9
        
        # キーワード数とテキスト長の比率を計算
        total_keywords = sum(len(cat.keywords_found) for cat in categories)
        
        # 短いテキストで毒性キーワードが見つかった場合は信頼度高
        if text_length < 10 and total_keywords > 0:
            return 1.0
        
        # テキスト10文字あたりのキーワード数
        keyword_density = total_keywords / max(text_length / 10, 1)
        
        # 信頼度（0.7〜1.0の範囲に調整）
        confidence = min(0.7 + (keyword_density * 0.3), 1.0)
        
        return confidence
